{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_tf_old.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ncfMpJIZo_Mo","executionInfo":{"status":"ok","timestamp":1612059946553,"user_tz":300,"elapsed":24889,"user":{"displayName":"Biplav Choudhury","photoUrl":"","userId":"02301094446737336443"}},"outputId":"88719bd8-2c33-4320-9ef8-c7227e4552d2"},"source":["from collections import UserList\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import tqdm\n","import os\n","import pickle\n","\n","import datetime\n","import copy\n","import time\n","\n","# from tf_environment import *\n","# from comet_ml import Experiment\n","\n","# experiment = Experiment(\"HsbMT2nT816RPUXC1LLkVvEe0\")\n","\n","now = datetime.datetime.now()\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('drive/My Drive/Colab Notebooks/distributed_AoI_exp1-OLD/')\n","\n","!pwd\n","!pip install tf-agents\n","!pip install dm-reverb[tensorflow]\n","\n","from create_graph_1 import *\n","# from path_loss_probability import *\n","import itertools\n","from itertools import product  \n","from tf_reinforce import *\n","from tf_dqn import *\n","from tf_c51 import *\n","from tf_sac import *\n","from random_scheduling import *\n","from greedy_scheduling import *\n","from mad_scheduling import *\n","\n","\n","from joblib import Parallel, delayed\n","import multiprocessing as mp\n","\n","from parameters import *\n","\n","random.seed(42)\n","# tf.random.set_seed(42)\n","\n","def distributed_run(arguments):\n","  \n","    print(f\"passed arguments are {arguments}\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","    print(f\"passed arguments are {arguments}\")\n","\n","    # pool.starmap(do_scheduling, [(arg[0], arg[1], arg[2]) for arg in arguments]) ## this enable multiprocessing but I am getting memroy allocation and other CUDA related errors with this, so now using sequential execution\n","    \n","    for j in arguments:\n","        do_scheduling(j[0],j[1],j[2])\n","    \n","#############################################################\n","\n","def do_scheduling(deployment, I, scheduler):\n","    \n","    \n","    deployment_options = [\"MDS\", \"RP\"]\n","    scheduler_options  = [\"random\", \"greedy\", \"MAD\", \"dqn\", \"c51\", \"sac\"]\n","    assert(deployment in deployment_options and scheduler in scheduler_options)\n","\n","    random.seed(42) ## this seed ensures same location of users in every case, keep both seeds\n","    \n","    if test_case:\n","        \n","        ## exp 24\n","        print(f\"under experiment {experiment}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","\n","        drones_needed           = 1\n","        users_per_drone         = [5]  ## biplav\n","        adj_matrix              = np.array([[0, 1, 1, 0, 0],\n","                                            [0, 0, 1, 1, 0],\n","                                            [0, 0, 0, 1, 1],\n","                                            [1, 0, 0, 0, 1],\n","                                            [1, 1, 0, 0, 0]])\n","        # adj_matrix              = np.array([[0, 1, 1],\n","        #                                     [1, 0, 1],\n","        #                                     [1, 1, 0]])\n","        \n","        tx_rx_pairs = []\n","        tx_users    = []\n","        \n","        rows, columns = np.shape(adj_matrix)\n","        # print(f\"rows = {rows}, columns = {columns}\")\n","        \n","        ## relevant pair calculation starts\n","        \n","        # age at the final dest will be w.r.t only these pairs.  \n","        for i in range(rows):\n","            for ii in range(columns):\n","                if adj_matrix[i,ii]==1:\n","                    pair = [i + 10, ii + 10] ## 10 as count is 10 from main_tf.py where user IDs start from 10\n","                    tx_rx_pairs.append(pair)\n","        \n","        for i in tx_rx_pairs:\n","            if i[0] not in tx_users:\n","                tx_users.append(i[0])\n","        assert drones_needed    ==len(users_per_drone)\n","        \n","        drones_coverage         = []\n","        \n","        count = 10 # user IDs will start from this. and this also ensured that UAV and users have different IDs. Ensure number of UAVs is less than the count\n","        for i in range(drones_needed):\n","            individual_drone_coverage = [x for x in range(count, count + users_per_drone[i])]\n","            print(f\"individual_drone_coverage = {individual_drone_coverage}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","            count = individual_drone_coverage[-1] + 1\n","            drones_coverage.append(individual_drone_coverage)\n","            \n","        user_list = []\n","        UAV_list = np.arange(drones_needed)\n","        for i in drones_coverage:\n","            for j in i:\n","                if j!=0: ## user will not contain 0\n","                    user_list.append(j)\n","        \n","        print(f\"user_list = {user_list}, UAV_list = {UAV_list}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","        assert (max(user_list) - min(user_list))+1 == sum(users_per_drone)\n","        # time.sleep(10)\n","\n","                    \n","        if periodic_generation:\n","            periodicity = {10:2,11:3,12:4,13:2,14:3} #{x:random.choice([2,3,4]) for x in user_list}\n","        else:\n","            periodicity = {x:1 for x in user_list}\n","        \n","        I = len(user_list) # changed to the needed value\n","\n","        if packet_loss == True:\n","            packet_update_loss  = {tuple(yy) : round(random.random(),2) for yy in tx_rx_pairs}\n","            packet_sample_loss  = {yy : round(random.random(),2) for yy in user_list}\n","        else:\n","            packet_update_loss  = {tuple(yy) : -1 for yy in tx_rx_pairs}\n","            packet_sample_loss  = {yy : -1 for yy in user_list}\n","            \n","    else: ## user defined UAV and user configuration\n","        \n","        assert test_case == True, \"Test Case has to be true here\" # denominator can't be 0 \n","                        \n","        # I is number of users, L length and B breadth\n","        x_vals = random.sample(range(1, L-1), I) # x-coordinates for users\n","        y_vals = random.sample(range(1, B-1), I) # y-coordinates for users\n","        z_vals = [0]*I\n","\n","        user_coordinates = list(zip(x_vals,y_vals))\n","\n","        x_grid_nos = int(L/r) + 1 # number of different values the grid takes for x axis\n","        y_grid_nos = int(B/r) + 1 # number of different values the grid takes for y axis\n","\n","        grid_x = np.linspace(0, L, num = x_grid_nos) # generate evenly spaced x positions for grid\n","        grid_y = np.linspace(0, B, num = y_grid_nos) # generate evenly spaced y positions for grid\n","        \n","        grid_coordinates = list(itertools.product(grid_x , grid_y))\n","\n","        drones_needed, drones_coverage = create_graph_1(user_coordinates, grid_coordinates, deployment)       \n","    \n","        user_list = [] ## this is not the same user_list as defined in the environment, this is just used to index the packet loss and sample loss\n","        UAV_list  = np.arange(drones_needed)\n","        \n","        for i in drones_coverage:\n","            for j in i:\n","                if j!=0:\n","                    user_list.append(j)\n","                    \n","         \n","        if periodic_generation:\n","            periodicity = {x:random.choice([2,3,4]) for x in user_list}\n","        else:\n","            periodicity = {x:1 for x in user_list}\n","\n","\n","        if packet_loss == True:\n","            packet_update_loss = {yy : round(random.random(),2) for yy in user_list}\n","            packet_sample_loss = {yy : round(random.random(),2) for yy in user_list}\n","        else:\n","            packet_update_loss = {yy : -1 for yy in user_list}\n","            packet_sample_loss = {yy : -1 for yy in user_list}\n","            \n","\n","    print(f\"\\n\\n{deployment} deployment for {I} users under {scheduler} scheduling\\n\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","\n","            \n","    print(f'Under test_case = {test_case}, drones_needed = {drones_needed}, UAV_list = {UAV_list}, drones_coverage = {drones_coverage}, user_list = {user_list}, periodicity = {periodicity} for {deployment} deployment for {I} users under {scheduler} scheduling, update loss = {packet_update_loss}, sampling loss = {packet_sample_loss}, user_list = {user_list}, UAV_list = {UAV_list}, CSI_as_state = {CSI_as_state}, sample_error_in_CSI = {sample_error_in_CSI}\\n', file=open(folder_name + \"/results.txt\", \"a\"), flush=True)  \n","    \n","\n","    str_x = str(deployment) + \" placement with \" + str(I) + \" users needs \" + str(scheduler) + \" scheduler and \"  + str(drones_needed) + \" drones\\n\"\n","    print(f'{str_x}', file=open(folder_name + \"/drones.txt\", \"a\"), flush=True)\n","    \n","    \n","    if scheduler == \"greedy\":\n","        t1 = time.time()\n","        #with tf.device('/CPU:0'):\n","        greedy_overall[I], greedy_final[I], greedy_all_actions[I] = greedy_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)  \n","        t2 = time.time()\n","        print(\"greedy for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(greedy_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_overall.pickle\", \"wb\")) \n","        pickle.dump(greedy_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_final.pickle\", \"wb\"))\n","        pickle.dump(greedy_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"_greedy_all_actions.pickle\", \"wb\")) \n","    \n","    if scheduler == \"random\":\n","        t1 = time.time()\n","        #with tf.device('/CPU:0'):\n","        random_overall[I], random_final[I], random_all_actions[I] = random_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n","        t2 = time.time()\n","        print(\"random for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(random_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_overall.pickle\", \"wb\")) \n","        pickle.dump(random_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_final.pickle\", \"wb\")) \n","        pickle.dump(random_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_all_actions.pickle\", \"wb\")) \n","        \n","        \n","    if scheduler == \"MAD\":\n","        t1 = time.time()\n","        #with tf.device('/CPU:0'):\n","        mad_overall[I], mad_final[I], mad_all_actions[I] = mad_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n","        t2 = time.time()\n","        print(\"MAD for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(mad_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_overall.pickle\", \"wb\")) \n","        pickle.dump(mad_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_final.pickle\", \"wb\"))\n","        pickle.dump(mad_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_all_actions.pickle\", \"wb\")) \n","        \n","    \n","\n","    if scheduler == \"dqn\":\n","        t1 = time.time()\n","        dqn_overall[I], dqn_final[I], dqn_all_actions[I] = tf_dqn(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n","        t2 = time.time()\n","        print(\"DQN for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(dqn_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_overall.pickle\", \"wb\")) \n","        pickle.dump(dqn_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_final.pickle\", \"wb\"))\n","        pickle.dump(dqn_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_all_actions.pickle\", \"wb\"))\n","\n","\n","    if scheduler == \"c51\":\n","        t1 = time.time()\n","        c51_overall[I], c51_final[I], c51_all_actions[I] = tf_c51(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix)\n","        t2 = time.time()\n","        print(\"c51 for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(c51_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_overall.pickle\", \"wb\")) \n","        pickle.dump(c51_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_final.pickle\", \"wb\")) \n","        pickle.dump(c51_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_all_actions.pickle\", \"wb\")) \n","\n","    if scheduler == \"sac\":\n","        t1 = time.time()\n","        #with tf.device('/CPU:0'):\n","        greedy_overall[I], greedy_final[I], greedy_all_actions[I] = greedy_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)  \n","        t2 = time.time()\n","        print(\"sac for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","        pickle.dump(greedy_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_overall.pickle\", \"wb\")) \n","        pickle.dump(greedy_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_final.pickle\", \"wb\"))\n","        pickle.dump(greedy_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"_greedy_all_actions.pickle\", \"wb\")) \n","        \n","\n","    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n","    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\")\n","\n","#############################################################\n","    \n","if __name__ == '__main__':\n","        \n","\n","    now_str_1 = now.strftime(\"%Y-%m-%d %H:%M\")\n","    folder_name = 'models/' +  now_str_1\n","    \n","    # print(f\"\\n\\nSTATUS OF GPU : {tf.test.is_built_with_gpu_support() and {tf.test.is_gpu_available()}}\\n\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","    \n","    # print(f\"\\n\\nSTATUS OF GPU : {tf.test.is_built_with_gpu_support() and {tf.test.is_gpu_available()}}\\n\\n\")\n","    \n","    folder_name_MDS = folder_name + \"/MDS\"\n","    folder_name_random = folder_name + \"/RP\" ## RP means random placement\n","\n","    if not os.path.isdir(folder_name):\n","        os.makedirs(folder_name)\n","        os.makedirs(folder_name_MDS)\n","        os.makedirs(folder_name_random) \n","        \n","    print(\"execution started at \", now_str_1, file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","\n","    print(\"num_iterations = \",num_iterations, \", random_episodes = \", random_episodes,\", DL_capacity = \", DL_capacity, \", UL_capacity = \", UL_capacity,\",  MAX_STEPS = \", MAX_STEPS, \" gamma = \", set_gamma, \", learning_rate = \", learning_rate, \", fc_layer_params = \", fc_layer_params, \", replay_buffer_capacity = \", replay_buffer_capacity, \", coverage_capacity = \", coverage_capacity, \", L = \", L, \", B = \", B, \", R = \", R, \", r = \", r,  \"\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n","\n","# export LD_LIBRARY_PATH = /path/to/conda/envs/tf1/lib conda create --name tf1 --clone tf\n","\n","    deployments = [\"RP\"] #, \"RP\"] #, \"MDS\"]\n","    \n","    schedulers  = [\"sac\"] ##     scheduler_options  = [\"random\", \"greedy\", \"MAD\", \"dqn\", \"c51\", \"sac\"]\n","    \n","    limit_memory = False ## enabling this makes the code not being able to find CUDA device\n","    \n","#############################\n","\n","    experiment = 1\n","\n","    if experiment == 1:\n","        test_case           = True\n","        packet_loss         = False\n","        periodic_generation = False\n","\n","    elif experiment == 2:\n","        test_case           = True\n","        packet_loss         = True\n","        periodic_generation = False\n","        \n","    elif experiment == 3:\n","        test_case           = False\n","        packet_loss         = False\n","        periodic_generation = False\n","\n","    elif experiment == 4:\n","        test_case           = False\n","        packet_loss         = True\n","        periodic_generation = False\n","\n","    elif experiment == 5:\n","        test_case           = True\n","        packet_loss         = False\n","        periodic_generation = True\n","\n","    elif experiment == 6:\n","        test_case           = True\n","        packet_loss         = True\n","        periodic_generation = True\n","        \n","    elif experiment == 7:\n","        test_case           = False\n","        packet_loss         = False\n","        periodic_generation = True\n","\n","    elif experiment == 8:\n","        test_case           = False\n","        packet_loss         = True\n","        periodic_generation = True\n","    \n","    if test_case:\n","        users = [5] ## will get changed accordingly inside the loop above # biplav\n","    else:\n","        users = [8,10]\n","\n","#############################\n","\n","    \n","    arguments = list(itertools.product(deployments, users, schedulers)) ## deployment, I, scheduler\n","    \n","    dqn_overall = {}\n","    dqn_final = {}\n","    dqn_all_actions = {}\n","    \n","    c51_overall = {}\n","    c51_final = {}\n","    c51_all_actions = {}\n","    \n","    reinforce_overall = {}\n","    reinforce_final = {}\n","    reinforce_all_actions = {}\n","    \n","    random_overall = {} ## sum of age at destination nodes for all of the MAX_STEPS time steps\n","    random_final   = {} ## sum of age at destination nodes for step =  MAX_STEPS i.e. last time step\n","    random_all_actions = {}\n","    \n","    greedy_overall = {}\n","    greedy_final   = {}\n","    greedy_all_actions = {}\n","    \n","    mad_overall = {}\n","    mad_final   = {}\n","    mad_all_actions = {}\n","\n","    pool = mp.Pool(mp.cpu_count())\n","    print(f\"pool is {pool} \\n\\n\", file = open(folder_name + \"/results.txt\", \"a\"))\n","    print(f\"experiment is {experiment} with test_case = {test_case}, packet_loss = {packet_loss}, periodic_generation = {periodic_generation}\", file = open(folder_name + \"/results.txt\", \"a\"))\n","    print(f\"experiment is {experiment} with test_case = {test_case}, packet_loss = {packet_loss}, periodic_generation = {periodic_generation}\")\n","\n","    distributed_run(arguments)\n","\n","    pool.close()    \n","\n","\n","from google.colab import files\n","!ls\n","!zip -r /content/sac.zip /content/models\n","# !ls\n","files.download(\"/content/sac.zip\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","Requirement already satisfied: tf-agents in /usr/local/lib/python3.6/dist-packages (0.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.7.4.3)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.4.0)\n","Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (7.0.0)\n","Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.12.1)\n","Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.17.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.1)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.4)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.3.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.19.5)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (51.3.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n","Requirement already satisfied: dm-reverb[tensorflow] in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from dm-reverb[tensorflow]) (0.1.5)\n","Requirement already satisfied: portpicker in /usr/local/lib/python3.6/dist-packages (from dm-reverb[tensorflow]) (1.3.1)\n","Requirement already satisfied: tensorflow>=2.4.0; extra == \"tensorflow\" in /usr/local/lib/python3.6/dist-packages (from dm-reverb[tensorflow]) (2.4.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from dm-tree->dm-reverb[tensorflow]) (1.15.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.3.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.7.4.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.6.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.12.4)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.10.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2.4.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.32.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2.4.1)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.12.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.3.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.36.2)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2.10.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.2.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.1.2)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.12)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (51.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.17.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.4.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.3.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (4.7)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (1.24.3)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0; extra == \"tensorflow\"->dm-reverb[tensorflow]) (3.4.0)\n","experiment is 1 with test_case = True, packet_loss = False, periodic_generation = False\n","passed arguments are [('RP', 5, 'sac')]\n","\n","\n","greedy started for 5 users , coverage = [[10, 11, 12, 13, 14]] with update_loss = {(10, 11): -1, (10, 12): -1, (11, 12): -1, (11, 13): -1, (12, 13): -1, (12, 14): -1, (13, 10): -1, (13, 14): -1, (14, 10): -1, (14, 11): -1}, sample_loss = {10: -1, 11: -1, 12: -1, 13: -1, 14: -1}, periodicity = {10: 1, 11: 1, 12: 1, 13: 1, 14: 1}, tx_rx_pairs = [[10, 11], [10, 12], [11, 12], [11, 13], [12, 13], [12, 14], [13, 10], [13, 14], [14, 10], [14, 11]], tx_users = [10, 11, 12, 13, 14]  and RP deployment, UL_capacity = 2, DL_capacity = 1 and RP deployment\n","\n","greedy scheduling  RP  placement,  5  users - avg of final_step_rewards =  71.9659  MIN and MAX of final_step_rewards =  65 ,  80  and avg of overall_ep_reward =  518.9049  : end with final state of  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]  with shape  (16,)\n","5 users under sac scheduling and RP placement are over\n","\n","\n","drive  models  sample_data\n","  adding: content/models/ (stored 0%)\n","  adding: content/models/2021-01-31 02:25/ (stored 0%)\n","  adding: content/models/2021-01-31 02:25/action_space.txt (deflated 85%)\n","  adding: content/models/2021-01-31 02:25/MDS/ (stored 0%)\n","  adding: content/models/2021-01-31 02:25/RP/ (stored 0%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_overall.pickle (deflated 76%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_success_sample.pickle (deflated 100%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_age_dist_dest.pickle (deflated 73%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_UAV_returns.pickle (deflated 100%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_attempt_sample.pickle (deflated 100%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_attempt_update.pickle (deflated 100%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_final.pickle (deflated 78%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_age_dist_UAV.pickle (deflated 86%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_success_update.pickle (deflated 100%)\n","  adding: content/models/2021-01-31 02:25/RP/5_greedy_all_actions.pickle (deflated 53%)\n","  adding: content/models/2021-01-31 02:25/RP/5U_greedy_time.pickle (stored 0%)\n","  adding: content/models/2021-01-31 02:25/drones.txt (stored 0%)\n","  adding: content/models/2021-01-31 02:25/results.txt (deflated 63%)\n","  adding: content/models/2021-01-31 02:21/ (stored 0%)\n","  adding: content/models/2021-01-31 02:21/action_space.txt (deflated 85%)\n","  adding: content/models/2021-01-31 02:21/MDS/ (stored 0%)\n","  adding: content/models/2021-01-31 02:21/RP/ (stored 0%)\n","  adding: content/models/2021-01-31 02:21/drones.txt (stored 0%)\n","  adding: content/models/2021-01-31 02:21/results.txt (deflated 63%)\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_d085f951-8a11-4a64-826d-2df07f558988\", \"sac.zip\", 317291)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}
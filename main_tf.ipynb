{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_tf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncfMpJIZo_Mo",
        "outputId": "49ddd822-98fa-41b4-b204-6e321cd432ef"
      },
      "source": [
        "from collections import UserList\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tqdm\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import datetime\n",
        "import copy\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# from tf_environment import *\n",
        "# from comet_ml import Experiment\n",
        "\n",
        "# experiment = Experiment(\"HsbMT2nT816RPUXC1LLkVvEe0\")\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/distributed_AoI_exp1/')\n",
        "\n",
        "!pwd\n",
        "!pip install tf-agents\n",
        "!pip install dm-reverb[tensorflow]\n",
        "\n",
        "from create_graph_1 import *\n",
        "# from path_loss_probability import *\n",
        "import itertools\n",
        "from itertools import product  \n",
        "from tf_reinforce import *\n",
        "from tf_dqn import *\n",
        "from tf_c51 import *\n",
        "from tf_sac import *\n",
        "from random_scheduling import *\n",
        "from greedy_scheduling import *\n",
        "from mad_scheduling import *\n",
        "\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing as mp\n",
        "\n",
        "from parameters import *\n",
        "\n",
        "random.seed(42)\n",
        "# tf.random.set_seed(42)\n",
        "\n",
        "def distributed_run(arguments):\n",
        "  \n",
        "    print(f\"passed arguments are {arguments}\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "    print(f\"passed arguments are {arguments}\")\n",
        "\n",
        "    # pool.starmap(do_scheduling, [(arg[0], arg[1], arg[2]) for arg in arguments]) ## this enable multiprocessing but I am getting memroy allocation and other CUDA related errors with this, so now using sequential execution\n",
        "    \n",
        "    for j in arguments:\n",
        "        do_scheduling(j[0],j[1],j[2])\n",
        "    \n",
        "#############################################################\n",
        "\n",
        "def do_scheduling(deployment, I, scheduler):\n",
        "    \n",
        "    \n",
        "    deployment_options = [\"MDS\", \"RP\"]\n",
        "    scheduler_options  = [\"random\", \"greedy\", \"MAD\", \"dqn\", \"c51\", \"sac\"]\n",
        "    assert(deployment in deployment_options and scheduler in scheduler_options)\n",
        "\n",
        "    random.seed(42) ## this seed ensures same location of users in every case, keep both seeds\n",
        "    \n",
        "    if test_case:\n",
        "        \n",
        "        ## exp 24\n",
        "        print(f\"under experiment {experiment}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "\n",
        "        drones_needed           = 1\n",
        "        users_per_drone         = [5] ## biplav\n",
        "        adj_matrix              = np.array([[0, 1, 1, 0, 0],\n",
        "                                            [0, 0, 1, 1, 0],\n",
        "                                            [0, 0, 0, 1, 1],\n",
        "                                            [1, 0, 0, 0, 1],\n",
        "                                            [1, 1, 0, 0, 0]])\n",
        "        # adj_matrix              = np.array([[0, 1, 1],\n",
        "        #                                     [1, 0, 1],\n",
        "        #                                     [1, 1, 0]])\n",
        "        \n",
        "        tx_rx_pairs = []\n",
        "        tx_users    = []\n",
        "        \n",
        "        rows, columns = np.shape(adj_matrix)\n",
        "        # print(f\"rows = {rows}, columns = {columns}\")\n",
        "        \n",
        "        ## relevant pair calculation starts\n",
        "        \n",
        "        # age at the final dest will be w.r.t only these pairs.  \n",
        "        for i in range(rows):\n",
        "            for ii in range(columns):\n",
        "                if adj_matrix[i,ii]==1:\n",
        "                    pair = [i + 10, ii + 10] ## 10 as count is 10 from main_tf.py where user IDs start from 10\n",
        "                    tx_rx_pairs.append(pair)\n",
        "        \n",
        "        for i in tx_rx_pairs:\n",
        "            if i[0] not in tx_users:\n",
        "                tx_users.append(i[0])\n",
        "        assert drones_needed    ==len(users_per_drone)\n",
        "        \n",
        "        drones_coverage         = []\n",
        "        \n",
        "        count = 10 # user IDs will start from this. and this also ensured that UAV and users have different IDs. Ensure number of UAVs is less than the count\n",
        "        for i in range(drones_needed):\n",
        "            individual_drone_coverage = [x for x in range(count, count + users_per_drone[i])]\n",
        "            print(f\"individual_drone_coverage = {individual_drone_coverage}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "            count = individual_drone_coverage[-1] + 1\n",
        "            drones_coverage.append(individual_drone_coverage)\n",
        "            \n",
        "        user_list = []\n",
        "        UAV_list = np.arange(drones_needed)\n",
        "        for i in drones_coverage:\n",
        "            for j in i:\n",
        "                if j!=0: ## user will not contain 0\n",
        "                    user_list.append(j)\n",
        "        \n",
        "        print(f\"user_list = {user_list}, UAV_list = {UAV_list}\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "        assert (max(user_list) - min(user_list))+1 == sum(users_per_drone)\n",
        "        # time.sleep(10)\n",
        "\n",
        "                    \n",
        "        if periodic_generation:\n",
        "            periodicity = {10:2,11:3,12:4,13:2,14:3} #{x:random.choice([2,3,4]) for x in user_list}\n",
        "        else:\n",
        "            periodicity = {x:1 for x in user_list}\n",
        "        \n",
        "        I = len(user_list) # changed to the needed value\n",
        "\n",
        "        if packet_loss == True:\n",
        "            packet_update_loss  = {tuple(yy) : round(random.random(),2) for yy in tx_rx_pairs}\n",
        "            packet_sample_loss  = {yy : round(random.random(),2) for yy in user_list}\n",
        "        else:\n",
        "            packet_update_loss  = {tuple(yy) : -1 for yy in tx_rx_pairs}\n",
        "            packet_sample_loss  = {yy : -1 for yy in user_list}\n",
        "            \n",
        "    else: ## user defined UAV and user configuration\n",
        "        \n",
        "        assert test_case == True, \"Test Case has to be true here\" # denominator can't be 0 \n",
        "                        \n",
        "        # I is number of users, L length and B breadth\n",
        "        x_vals = random.sample(range(1, L-1), I) # x-coordinates for users\n",
        "        y_vals = random.sample(range(1, B-1), I) # y-coordinates for users\n",
        "        z_vals = [0]*I\n",
        "\n",
        "        user_coordinates = list(zip(x_vals,y_vals))\n",
        "\n",
        "        x_grid_nos = int(L/r) + 1 # number of different values the grid takes for x axis\n",
        "        y_grid_nos = int(B/r) + 1 # number of different values the grid takes for y axis\n",
        "\n",
        "        grid_x = np.linspace(0, L, num = x_grid_nos) # generate evenly spaced x positions for grid\n",
        "        grid_y = np.linspace(0, B, num = y_grid_nos) # generate evenly spaced y positions for grid\n",
        "        \n",
        "        grid_coordinates = list(itertools.product(grid_x , grid_y))\n",
        "\n",
        "        drones_needed, drones_coverage = create_graph_1(user_coordinates, grid_coordinates, deployment)       \n",
        "    \n",
        "        user_list = [] ## this is not the same user_list as defined in the environment, this is just used to index the packet loss and sample loss\n",
        "        UAV_list  = np.arange(drones_needed)\n",
        "        \n",
        "        for i in drones_coverage:\n",
        "            for j in i:\n",
        "                if j!=0:\n",
        "                    user_list.append(j)\n",
        "                    \n",
        "         \n",
        "        if periodic_generation:\n",
        "            periodicity = {x:random.choice([2,3,4]) for x in user_list}\n",
        "        else:\n",
        "            periodicity = {x:1 for x in user_list}\n",
        "\n",
        "\n",
        "        if packet_loss == True:\n",
        "            packet_update_loss = {yy : round(random.random(),2) for yy in user_list}\n",
        "            packet_sample_loss = {yy : round(random.random(),2) for yy in user_list}\n",
        "        else:\n",
        "            packet_update_loss = {yy : -1 for yy in user_list}\n",
        "            packet_sample_loss = {yy : -1 for yy in user_list}\n",
        "            \n",
        "\n",
        "    print(f\"\\n\\n{deployment} deployment for {I} users under {scheduler} scheduling\\n\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "\n",
        "            \n",
        "    print(f'Under test_case = {test_case}, drones_needed = {drones_needed}, UAV_list = {UAV_list}, drones_coverage = {drones_coverage}, user_list = {user_list}, periodicity = {periodicity} for {deployment} deployment for {I} users under {scheduler} scheduling, update loss = {packet_update_loss}, sampling loss = {packet_sample_loss}, user_list = {user_list}, UAV_list = {UAV_list}, CSI_as_state = {CSI_as_state}, sample_error_in_CSI = {sample_error_in_CSI}\\n', file=open(folder_name + \"/results.txt\", \"a\"), flush=True)  \n",
        "    \n",
        "\n",
        "    str_x = str(deployment) + \" placement with \" + str(I) + \" users needs \" + str(scheduler) + \" scheduler and \"  + str(drones_needed) + \" drones\\n\"\n",
        "    print(f'{str_x}', file=open(folder_name + \"/drones.txt\", \"a\"), flush=True)\n",
        "    \n",
        "    \n",
        "    if scheduler == \"greedy\":\n",
        "        t1 = time.time()\n",
        "        #with tf.device('/CPU:0'):\n",
        "        greedy_overall[I], greedy_final[I], greedy_all_actions[I] = greedy_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)  \n",
        "        t2 = time.time()\n",
        "        print(\"greedy for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(greedy_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(greedy_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_final.pickle\", \"wb\"))\n",
        "        pickle.dump(greedy_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"_greedy_all_actions.pickle\", \"wb\")) \n",
        "    \n",
        "    if scheduler == \"random\":\n",
        "        t1 = time.time()\n",
        "        #with tf.device('/CPU:0'):\n",
        "        random_overall[I], random_final[I], random_all_actions[I] = random_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n",
        "        t2 = time.time()\n",
        "        print(\"random for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(random_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(random_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_final.pickle\", \"wb\")) \n",
        "        pickle.dump(random_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_all_actions.pickle\", \"wb\")) \n",
        "        \n",
        "        \n",
        "    if scheduler == \"MAD\":\n",
        "        t1 = time.time()\n",
        "        #with tf.device('/CPU:0'):\n",
        "        mad_overall[I], mad_final[I], mad_all_actions[I] = mad_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n",
        "        t2 = time.time()\n",
        "        print(\"MAD for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(mad_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(mad_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_final.pickle\", \"wb\"))\n",
        "        pickle.dump(mad_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_all_actions.pickle\", \"wb\")) \n",
        "        \n",
        "    \n",
        "\n",
        "    if scheduler == \"dqn\":\n",
        "        t1 = time.time()\n",
        "        dqn_overall[I], dqn_final[I], dqn_all_actions[I] = tf_dqn(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n",
        "        t2 = time.time()\n",
        "        print(\"DQN for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(dqn_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(dqn_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_final.pickle\", \"wb\"))\n",
        "        pickle.dump(dqn_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_all_actions.pickle\", \"wb\"))\n",
        "\n",
        "\n",
        "    if scheduler == \"c51\":\n",
        "        t1 = time.time()\n",
        "        c51_overall[I], c51_final[I], c51_all_actions[I] = tf_c51(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)\n",
        "        t2 = time.time()\n",
        "        print(\"c51 for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(c51_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(c51_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_final.pickle\", \"wb\"))\n",
        "        pickle.dump(c51_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_all_actions.pickle\", \"wb\"))\n",
        "\n",
        "    if scheduler == \"sac\":\n",
        "        t1 = time.time()\n",
        "        sac_overall[I], sac_final[I], sac_all_actions[I] = tf_sac(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity, adj_matrix, tx_rx_pairs, tx_users)  \n",
        "        t2 = time.time()\n",
        "        print(\"sac for \", I, \" users took \", t2-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "        pickle.dump(sac_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_sac_overall.pickle\", \"wb\")) \n",
        "        pickle.dump(sac_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_sac_final.pickle\", \"wb\"))\n",
        "        pickle.dump(sac_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"_sac_all_actions.pickle\", \"wb\")) \n",
        "        \n",
        "\n",
        "    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
        "    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\")\n",
        "\n",
        "#############################################################\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "        \n",
        "\n",
        "    now_str_1 = now.strftime(\"%Y-%m-%d %H:%M\")\n",
        "    folder_name = 'models/' +  now_str_1\n",
        "    \n",
        "    # print(f\"\\n\\nSTATUS OF GPU : {tf.test.is_built_with_gpu_support() and {tf.test.is_gpu_available()}}\\n\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "    \n",
        "    # print(f\"\\n\\nSTATUS OF GPU : {tf.test.is_built_with_gpu_support() and {tf.test.is_gpu_available()}}\\n\\n\")\n",
        "    \n",
        "    folder_name_MDS = folder_name + \"/MDS\"\n",
        "    folder_name_random = folder_name + \"/RP\" ## RP means random placement\n",
        "\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "        os.makedirs(folder_name_MDS)\n",
        "        os.makedirs(folder_name_random) \n",
        "        \n",
        "    print(\"execution started at \", now_str_1, file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "\n",
        "    print(\"num_iterations = \",num_iterations, \", random_episodes = \", random_episodes,\", DL_capacity = \", DL_capacity, \", UL_capacity = \", UL_capacity,\",  MAX_STEPS = \", MAX_STEPS, \" gamma = \", set_gamma, \", learning_rate = \", learning_rate, \", fc_layer_params = \", fc_layer_params, \", replay_buffer_capacity = \", replay_buffer_capacity, \", coverage_capacity = \", coverage_capacity, \", L = \", L, \", B = \", B, \", R = \", R, \", r = \", r,  \"\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
        "\n",
        "    deployments = [\"RP\"] #, \"RP\"] #, \"MDS\"]\n",
        "    \n",
        "    schedulers  = [\"sac\"] ##     scheduler_options  = [\"random\", \"greedy\", \"MAD\", \"dqn\", \"c51\", \"sac\"]\n",
        "    \n",
        "    limit_memory = False ## enabling this makes the code not being able to find CUDA device\n",
        "    \n",
        "#############################\n",
        "\n",
        "    experiment = 1\n",
        "\n",
        "    if experiment == 1:\n",
        "        test_case           = True\n",
        "        packet_loss         = False\n",
        "        periodic_generation = False\n",
        "\n",
        "    elif experiment == 2:\n",
        "        test_case           = True\n",
        "        packet_loss         = True\n",
        "        periodic_generation = False\n",
        "        \n",
        "    elif experiment == 3:\n",
        "        test_case           = False\n",
        "        packet_loss         = False\n",
        "        periodic_generation = False\n",
        "\n",
        "    elif experiment == 4:\n",
        "        test_case           = False\n",
        "        packet_loss         = True\n",
        "        periodic_generation = False\n",
        "\n",
        "    elif experiment == 5:\n",
        "        test_case           = True\n",
        "        packet_loss         = False\n",
        "        periodic_generation = True\n",
        "\n",
        "    elif experiment == 6:\n",
        "        test_case           = True\n",
        "        packet_loss         = True\n",
        "        periodic_generation = True\n",
        "        \n",
        "    elif experiment == 7:\n",
        "        test_case           = False\n",
        "        packet_loss         = False\n",
        "        periodic_generation = True\n",
        "\n",
        "    elif experiment == 8:\n",
        "        test_case           = False\n",
        "        packet_loss         = True\n",
        "        periodic_generation = True\n",
        "    \n",
        "    if test_case:\n",
        "        users = [5] ## will get changed accordingly inside the loop above ## biplav\n",
        "    else:\n",
        "        users = [8,10]\n",
        "\n",
        "#############################\n",
        "\n",
        "    \n",
        "    arguments = list(itertools.product(deployments, users, schedulers)) ## deployment, I, scheduler\n",
        "    \n",
        "    dqn_overall = {}\n",
        "    dqn_final = {}\n",
        "    dqn_all_actions = {}\n",
        "    \n",
        "    c51_overall = {}\n",
        "    c51_final = {}\n",
        "    c51_all_actions = {}\n",
        "    \n",
        "    reinforce_overall = {}\n",
        "    reinforce_final = {}\n",
        "    reinforce_all_actions = {}\n",
        "    \n",
        "    random_overall = {} ## sum of age at destination nodes for all of the MAX_STEPS time steps\n",
        "    random_final   = {} ## sum of age at destination nodes for step =  MAX_STEPS i.e. last time step\n",
        "    random_all_actions = {}\n",
        "    \n",
        "    greedy_overall = {}\n",
        "    greedy_final   = {}\n",
        "    greedy_all_actions = {}\n",
        "    \n",
        "    mad_overall = {}\n",
        "    mad_final   = {}\n",
        "    mad_all_actions = {}\n",
        "    \n",
        "    sac_overall = {}\n",
        "    sac_final   = {}\n",
        "    sac_all_actions = {}\n",
        "\n",
        "    pool = mp.Pool(mp.cpu_count())\n",
        "    print(f\"pool is {pool} \\n\\n\", file = open(folder_name + \"/results.txt\", \"a\"))\n",
        "    print(f\"experiment is {experiment} with test_case = {test_case}, packet_loss = {packet_loss}, periodic_generation = {periodic_generation}\", file = open(folder_name + \"/results.txt\", \"a\"))\n",
        "    print(f\"experiment is {experiment} with test_case = {test_case}, packet_loss = {packet_loss}, periodic_generation = {periodic_generation}\")\n",
        "\n",
        "    distributed_run(arguments)\n",
        "\n",
        "    pool.close()    \n",
        "  \n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "!ls\n",
        "!zip -r /content/sac.zip /content/models\n",
        "# !ls\n",
        "files.download(\"/content/sac.zip\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.3; Detected an installation of version 2.1.0. Please upgrade TensorFlow to proceed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c57022131bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# !pip install dm-reverb[tensorflow]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_graph_1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m# from path_loss_probability import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/AoI/distributed_AoI_exp1/create_graph_1.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/AoI/distributed_AoI_exp1/tf_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_py_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tf_agents/environments/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# TODO(b/155801943): Bring parallel_py_environment here once we're py3-only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatched_py_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_py_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tf_agents/environments/batched_py_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[0;31m# pylint: disable=g-explicit-tensorflow-version-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectories\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectories\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tf_agents/specs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArraySpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoundedArraySpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributionSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoundedTensorSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tf_agents/specs/distribution_spec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m  \u001b[0;31m# pylint:disable=g-direct-tensorflow-import  # TF internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0m_ensure_tf_install\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Cleanup symbols to avoid polluting namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m_ensure_tf_install\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"Please upgrade TensorFlow to proceed.\".format(\n\u001b[1;32m     65\u001b[0m             \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired_tensorflow_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             present=tf.__version__))\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.3; Detected an installation of version 2.1.0. Please upgrade TensorFlow to proceed."
          ]
        }
      ]
    }
  ]
}